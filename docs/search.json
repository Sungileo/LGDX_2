[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " ",
    "section": "",
    "text": "텍스트마이닝\n\n\n\n\n\n텍스트마이닝을 해보아요\n\n\n\n\n\n\nJul 30, 2024\n\n\n박성일\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\ntest content page\n\n\n\n\n\n\nJul 23, 2024\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\ntest content page\n\n\n\n\n\n\nJul 23, 2024\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\ntest content page\n\n\n\n\n\n\nJul 23, 2024\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\ntest content page\n\n\n\n\n\n\nJul 23, 2024\n\n\nSungil Park\n\n\n\n\n\n\n  \n\n\n\n\n이직 여부 로지스틱 회귀분석\n\n\n\n\n\n로지스틱 회귀를 해보아요\n\n\n\n\n\n\nJul 19, 2024\n\n\nSungil Park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "반갑습니다",
    "section": "",
    "text": "반갑습니다\n\n\n박성일입니다."
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html",
    "href": "posts/텍스트마이닝_sample.html",
    "title": "텍스트마이닝",
    "section": "",
    "text": "텍스트 마이닝이란 대량의 텍스트 데이터에서 유용한 정보와 패턴을 추출하고 분석하는 과정이다.\n전체 데이터는 문장컬럼이 어떤 대상을 혐오하는지에 대한 데이터이고, 분류 분석을 통해 문장의 어떤 특징이 분류 결과에 가중치를 주었는지를 보고자 한다.\n\n\n\n\n토큰화란 텍스트 데이터 처리에서 중요한 첫 단계로 텍스트를 분석하기 위해 작은 단위로 분할하는 과정을 의미한다.\n토큰화의 목적은 텍스트를 단어, 문장, 구절 등 의미 있는 작은 단위로 나누어 기계가 이해할 수 있게 하는 것이다.\n\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_data = 'data/unsmile_train_v1.0.tsv'\ntrain = pd.read_csv(train_data, delimiter = '\\t')\n\ntest_data = 'data/unsmile_valid_v1.0.tsv'\ntest = pd.read_csv(test_data, delimiter = '\\t')\ntrain.head()\n\n\n\n\n\n\n\n\n문장\n여성/가족\n남성\n성소수자\n인종/국적\n연령\n지역\n종교\n기타 혐오\n악플/욕설\nclean\n개인지칭\n\n\n\n\n0\n일안하는 시간은 쉬고싶어서 그런게 아닐까\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o doin 진짜 띵...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nntlk는 자연어 처리에 사용되는 패키지이고, word_tokenize는 텍스트를 단어 단위로 분할하는데 사용된다.\npunkt는 ntlk에서 제공되는 패키지로, 텍스트를 토큰화하는데 필요한 알고리즘과 데이터가 있다.\n\nfrom nltk import word_tokenize\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /home/sungil/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\n“넌 바라는게 너무 많아, 우리 헤어져” 를 word_tokenize()로 토큰화하면 다음과 같다.\n\nwords = word_tokenize(\"넌 바라는게 너무 많아, 우리 헤어져\")\nwords\n\n['넌', '바라는게', '너무', '많아', ',', '우리', '헤어져']\n\n\n전체 문장 토큰화\n\nclean_token = []\nfor i in train['문장']:\n    for j in word_tokenize(i):\n        clean_token.append(j)\n# 9개 값만 뽑아보기\nclean_token[:9]\n\n['일안하는', '시간은', '쉬고싶어서', '그런게', '아닐까', '아동성범죄와', '페도버는', '기록바', '끊어져']\n\n\n토큰화된 텍스트 데이터에서 어떤 단어가 많이 나왔는지, 빈도를 세어 보고, 워드클라우드를 그려보았다.\nCounter() : 딕셔너리 형식으로 리스트 내의 값의 빈도 출력\n\nfrom collections import Counter\ncount = Counter(clean_token)\n# 상위 15개 값만 추출\ncount.most_common(15)\n\n[('?', 3815),\n ('.', 3416),\n ('!', 2066),\n (',', 1174),\n ('..', 979),\n ('...', 905),\n ('다', 667),\n ('진짜', 543),\n ('왜', 540),\n ('ㅋㅋ', 435),\n ('존나', 376),\n ('그냥', 372),\n ('더', 368),\n ('ㅋㅋㅋ', 363),\n ('&gt;', 296)]\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwc = WordCloud(background_color = 'white', font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\nwc.generate_from_frequencies(count)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n빈도를 세본 결과, 1글자(왜,다,더,좀..)와 “ㅋㅋㅋ”등 큰 의미가 없는 단어가 있었다.\n\n\n\n한글자 단어와, 기호, “ㅋㅋㅋ” 등 큰 의미가 없는 단어를 제거하면,\n텍스트 데이터의 노이즈를 줄이고, 더 정확하고 유용한 정보를 추출할 수 있다.\n\n텍스트 데이터의 전처리를 위해 re패키지를 사용한다.\n\nre패키지는 정규표현식을 사용하여 문자열을 조작하는 기능을 제공한다.\np = re.compile(pattern)은 토큰에 pattern이 포함되어 있는지를 출력한다(True/False)\n\"[ㅋㅎㄷㅇ~!?.\\\\-ㅡ0-9a-z]+\" 패턴은 ㅋ, ㅎ, ㄷ, ㅇ, ~, !, ?, ., -, ㅡ, 숫자(0~9), 소문자 영어(a~z) 를 의미한다.\n\n\n\n반복문을 사용하여 공백 기준으로 토큰화\n토큰이 한 글자거나 pattern에 해당하면 스킵\n스킵되지 않은 토큰들을 다시 문장으로 만들어 train clean 리스트에 넣기\n\n\nimport re\np = re.compile(\"[ㅋㅎㄷㅇ~!?.\\\\-ㅡ0-9a-z]+\")\ntrain_clean = [] \n\nfor doc in train['문장']:\n    temp = []\n    for token in doc.split(\" \"):\n        if len(token) &lt; 2:\n            continue\n        if p.search(token):\n            continue\n\n        temp.append(token)\n    train_clean.append(\" \".join(temp))\n\n# 테스트 데이터\ntest_clean = []\n\nfor doc in test['문장']:\n    temp = []\n    for token in doc.split(\" \"):\n        if len(token) &lt; 2:\n            continue\n        if p.search(token):\n            continue\n\n        temp.append(token)\n    test_clean.append(\" \".join(temp))\n\n텍스트 데이터가 더 깔끔해진 것을 확인할 수 있다.\n\ntrain_clean[:6]\n\n['일안하는 시간은 쉬고싶어서 그런게 아닐까',\n '아동성범죄와 페도버는 기록바 끊어져 영원히 고통 무슬림 IQ 떨어지고 출산 위험은',\n '루나 솔로앨범 나왔을 때부터 머모 기운 있었음 진짜 띵곡임 들어보셈\"',\n '홍팍에도 어버이연합인가 보내요 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽에서 고소',\n '아놔 여기 댓들은 여자들이 김치녀라고 먼저 여자들은 심하게 이렇게 내가 둘다 나쁜 이상도 이하도 아닌데',\n '고향가서 피방가면 동네 부럴 친구들이랑은 거르는 없이 이야기하니까 말하게 되더라 당연히 키보드를 치거나 그러지는 않는데 말하는게 많이 거칠어지긴 반성해야겠네']\n\n\n\n\n\n한국어 문법 특성상 텍스트 데이터를 공백으로 나누는 것보다, 형태소 단위로 나누면 텍스트의 의미를 더 정밀하게 분석할 수 있다.\n\n형태소 분리를 하기 위해 konlpy패키지를 사용한다.\nkonlpy 패키지에는 Hannanum, Kkma, Komoran, Mecab, Okt등의 형태소 분석기가 있다.\nmorphs()함수는 문장을 형태소 단위로 나누어준다.\n다음은 형태소 단위 분리의 예시이다.\n\nfrom konlpy.tag import Okt, Kkma\nokt = Okt()\n\nt_1 = \"아버지가 방에 들어가신다\"\nt_2 = \"아버지 가방에 들어가신다\"\n\nprint(okt.morphs(t_1))\nprint(okt.morphs(t_2))\n\n['아버지', '가', '방', '에', '들어가신다']\n['아버지', '가방', '에', '들어가신다']\n\n\n형태소 단위 분리에 앞서 emoji패키지를 사용해 문장 내의 이모지를 제거하였다.\n\nimport emoji\ntrain_clean = [emoji.replace_emoji(tc) for tc in train_clean]\ntest_clean = [emoji.replace_emoji(tc) for tc in test_clean]\n\nKkma형태소 분석기의 morphs() 함수를 사용하여 문장을 형태소로 분리, 리스트 형태로 저장.\ntrain : test = 2700 : 1000 으로 사용\n\nfrom tqdm import tqdm\n\nkkma = Kkma()\nkonlpy_morphs_train = []\nfor doc in tqdm(train_clean[:2700]):\n  rs = kkma.morphs(doc)\n  konlpy_morphs_train.append(\" \".join(rs))\n\n# 테스트트\nkonlpy_morphs_test = []\nfor doc in tqdm(test_clean[:1000]): \n  rs = kkma.morphs(doc)\n  konlpy_morphs_test.append(\" \".join(rs))\n\n  0%|          | 0/2700 [00:00&lt;?, ?it/s]100%|██████████| 2700/2700 [00:37&lt;00:00, 72.91it/s] \n100%|██████████| 1000/1000 [00:12&lt;00:00, 81.21it/s]\n\n\n\nkonlpy_morphs_train[:6]\n\n['일 안 하 는 시간 은 쉬 고 싶 어서 그러 ㄴ 것 이 아니 ㄹ까',\n '아동 성 범죄 와 페도버 는 기록 바 끊어지 어 영원히 고통 무슬림 IQ 떨어지 고 출산 위험 은',\n '루 나 솔로 앨범 나오 았 을 때 부터 머 모 기운 있 었 음 진짜 띵 곡 임 듣 어 보셈 \"',\n '홍팍에 도 어버이 연합 이 ㄴ가 보 내요 이러 ㄴ 뎃 글 있 는데 이거 어버이 연합 측 에 신고 하면 그쪽 에서 고소',\n '아 아 노 아 여기 댓 듣 은 여자 들 이 김치 녀 라고 먼저 여자 들 은 심하 게 이렇 게 내가 둘 다 나쁘 ㄴ 이상 도 이하 도 아니 ㄴ데',\n '고향 가 아서 피 방가 이 면 동네 부 럴 치 ㄴ 구들 이랑 은 거르 는 없이 이야기 하 니까 말하 게 되 더라 당연히 키보드 를 치 거나 그러 지 는 않 는데 말하 는 것 이 많이 거칠어지 기 는 반성 하 어야 겠네']\n\n\n\n\n\n\nkkma 형태소 분석기로 분리한 텍스트 데이터를, 모델에 적용하기 위해 숫자 형태로 바꾸어준다.\n\n\n\nBOW(Bag-of-Words) 는 텍스트 데이터에서 각 단어의 빈도를 세어 숫자형 벡터로 변환하는 방법이다.\nCountVectorizer는 BOW방식으로 텍스트 데이터를 숫자형 피처 벡터로 변환해주는 Scikit-learn의 클래스이다.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\nsample_bow = CountVectorizer()\nsample_bow.fit(sample_text)\n\nCountVectorizer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizerCountVectorizer()\n\n\n예시 문장으로 만든 단어 사전 sample_bow는 다음과 같다.\n\nsample_bow.vocabulary_\n\n{'나는': 1,\n '오늘': 9,\n '치킨을': 13,\n '먹을거야': 4,\n '너는': 2,\n '저녁에': 12,\n '무엇을': 5,\n '먹을거니': 3,\n '아침에': 7,\n '운동을': 11,\n '하고': 14,\n '왔어': 10,\n '어제': 8,\n '했더니': 15,\n '배가': 6,\n '고프다': 0}\n\n\n단어사전 16글자에서 어떤 단어가 등장했는지를 1로 표현하는 숫자 벡터이다.\n데이터프레임으로 변환한 단어사전과 비교해보면 이해할 수 있다.\n\npd.DataFrame([[i,j] for i,j in sample_bow.vocabulary_.items()], columns=[\"단어\",\"인덱스\"]).sort_values(\"인덱스\").reset_index(drop = True).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n단어\n고프다\n나는\n너는\n먹을거니\n먹을거야\n무엇을\n배가\n아침에\n어제\n오늘\n왔어\n운동을\n저녁에\n치킨을\n하고\n했더니\n\n\n인덱스\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n\n\n\n\nrs = sample_bow.transform(sample_text)\nrs.toarray()\n\narray([[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1]])\n\n\n\n\n\nTF-IDF는 텍스트를 숫자형 벡터로 변환하는 방법 중 하나로, 단어의 빈도(Term Frequency)에 가중치를 부여하는 방법이다.\n단순히 단어의 빈도만을 사용하는 Bag-of-Words 모델보다 더 유용한 정보를 제공한다.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\ntfidf_vectorizer = TfidfVectorizer()\n\ntfidf_vector = tfidf_vectorizer.fit_transform(sample_text)\n\n\ntfidf_vectorizer.vocabulary_\n\n{'나는': 1,\n '오늘': 9,\n '치킨을': 13,\n '먹을거야': 4,\n '너는': 2,\n '저녁에': 12,\n '무엇을': 5,\n '먹을거니': 3,\n '아침에': 7,\n '운동을': 11,\n '하고': 14,\n '왔어': 10,\n '어제': 8,\n '했더니': 15,\n '배가': 6,\n '고프다': 0}\n\n\n단어사전에 해당하는 인덱스가 0,1 이 아닌 가중치가 부여됨을 확인할 수 있다.\n\npd.DataFrame([[i,j] for i,j in tfidf_vectorizer.vocabulary_.items()], columns=[\"단어\",\"인덱스\"]).sort_values(\"인덱스\").reset_index(drop = True).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n단어\n고프다\n나는\n너는\n먹을거니\n먹을거야\n무엇을\n배가\n아침에\n어제\n오늘\n왔어\n운동을\n저녁에\n치킨을\n하고\n했더니\n\n\n인덱스\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n\n\n\n\nprint(tfidf_vector.toarray())\n\n[[0.         0.4530051  0.         0.         0.57457953 0.\n  0.         0.         0.         0.36674667 0.         0.\n  0.         0.57457953 0.         0.        ]\n [0.         0.         0.49819711 0.49819711 0.         0.49819711\n  0.         0.         0.         0.31799276 0.         0.\n  0.39278432 0.         0.         0.        ]\n [0.         0.36559366 0.         0.         0.         0.\n  0.         0.46370919 0.         0.29597957 0.46370919 0.36559366\n  0.         0.         0.46370919 0.        ]\n [0.43671931 0.         0.         0.         0.         0.\n  0.43671931 0.         0.43671931 0.         0.         0.34431452\n  0.34431452 0.         0.         0.43671931]]\n\n\n\n\n\nSentenceTransformer는 Transformer(영화아님, 딥러닝 모델) 기반의 사전 학습된 모델을 통하여 문장을 고차원 숫자 벡터로 변환한다.\nTF-IDF는 단어의 빈도에 가중치를 부여하는 방식으로, 문맥을 고려하지 못한다. ex)똥맛 카레, 카레맛 똥\nSentenceTransformer는 Transformer기반 모델을 통해 문맥을 고려하여 문장 내의 단어 순서와 의미를 반영할 수 있다.\n차원수의 차이 때문에 단순한 텍스트 분석인지, 복잡한 자연어처리 작업인지에 따라 사용하면 된다.\n사용모델 paraphrase-MiniLM-L6-v2\n\nfrom sentence_transformers import SentenceTransformer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\nsample_embedding = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\nembeddings = sample_embedding.encode(sample_text)\nembeddings\n\n2024-07-30 17:28:40.050781: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-07-30 17:28:40.051695: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-30 17:28:40.054281: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-30 17:28:40.061751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 17:28:40.074227: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 17:28:40.077846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-07-30 17:28:40.087182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-07-30 17:28:41.445550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\narray([[-0.0055915 ,  0.58696157,  0.12903269, ..., -0.5404554 ,\n         0.9031779 ,  0.508091  ],\n       [ 0.14074272,  0.4005823 ,  0.07539973, ..., -0.30134737,\n         0.9708954 ,  0.6109679 ],\n       [ 0.26552293,  0.540008  ,  0.1140722 , ..., -0.55974346,\n         0.8789244 ,  0.6666612 ],\n       [ 0.23060054,  0.45907632,  0.08265256, ..., -0.41313243,\n         1.0001894 ,  0.5942588 ]], dtype=float32)\n\n\n4 * 384의 행렬으로 변환되었다. (TF-IDF보다 큼)\n\nembeddings.shape\n\n(4, 384)\n\n\n\n\n\n\n분류 모델을 사용하여 숫자 벡터로 바꾼 텍스트 데이터가 무얼 혐오하는지 분류한다.\n앞서 사용한 텍스트 데이터의 변환 중 BOW(CountVectorizer), TF-IDF(TfidfVectorizer) 방식을 사용한다.\n파라미터의 의미는 다음과 같다.\n\nstop_words= : 숫자 벡터로 변환하는 과정에서 무시할 단어의 리스트\nngram_range= : 단어 묶음의 범위\n\nngram_range=1이면 [“오늘”, “치킨을”]\nngram_range=2 면 [“오늘 치킨을”]\n(1,2)는 [“오늘”, “치킨을”, “오늘 치킨을”] 이다\n\nmax_df= 특정 단어가 너무 자주 등장하면 무시하는 기능 (0.7 -&gt; 전체 문서의 70%이상이면 무시)\nmin_df= 특정 단어가 너무 적게 등장하면 무시하는 기능 (10 -&gt; 10개 이하는 무시)\n\n\n\n\nunsmile_bow = CountVectorizer(stop_words=[\"으로\",\"을\",\"를\",\"은\",\"이다\"],\n                              ngram_range = (1,2),\n                              max_df = 0.7, min_df = 10)\nunsmile_tfidf = TfidfVectorizer(stop_words=[\"으로\",\"을\",\"를\",\"은\",\"이다\"],\n                              ngram_range = (1,2),\n                              max_df = 0.7, min_df = 10)\n\n앞서 konlpy.Kkma를 사용해서 형태소 단위로 나는 데이터를 사용한다.\nfit_transform()메소드로 train 데이터를 학습하고, 변환하고\ntransform() 메소드로 test 데이터를 변환한다.\n\nX_train_bow = unsmile_bow.fit_transform(konlpy_morphs_train)\nX_test_bow = unsmile_bow.transform(konlpy_morphs_test)\n\nX_train_tfidf = unsmile_tfidf.fit_transform(konlpy_morphs_train)\nX_test_tfidf = unsmile_tfidf.transform(konlpy_morphs_test)\n\ny는 어떤 대상을 혐오한지의 인덱스 값이다.\n\nfor i,j in enumerate(train.loc[:, \"여성/가족\":\"clean\"].columns):\n    print(f\"인덱스[{i}] : {j}\")\n\n인덱스[0] : 여성/가족\n인덱스[1] : 남성\n인덱스[2] : 성소수자\n인덱스[3] : 인종/국적\n인덱스[4] : 연령\n인덱스[5] : 지역\n인덱스[6] : 종교\n인덱스[7] : 기타 혐오\n인덱스[8] : 악플/욕설\n인덱스[9] : clean\n\n\n\ny_train = train.loc[:, \"여성/가족\":\"clean\"].values.argmax(axis=1)\ny_test = test.loc[:, \"여성/가족\":\"clean\"].values.argmax(axis=1)\ny_train = y_train[:2700]\ny_test = y_test[:1000]\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_bow, y_train)\n\n# 예측\nbow_pred = logistic.predict(X_test_bow)\n\n# 분류 평가지표\nprint(classification_report(bow_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.35      0.49      0.41        78\n           1       0.59      0.69      0.64        65\n           2       0.51      0.64      0.57        59\n           3       0.43      0.73      0.54        66\n           4       0.19      1.00      0.32         6\n           5       0.55      0.76      0.64        42\n           6       0.58      0.81      0.67        37\n           7       0.00      0.00      0.00         5\n           8       0.38      0.34      0.36       218\n           9       0.74      0.45      0.56       424\n\n    accuracy                           0.50      1000\n   macro avg       0.43      0.59      0.47      1000\nweighted avg       0.57      0.50      0.51      1000\n\n\n\n\n\n\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_tfidf, y_train)\n\n# 예측\ntfidf_pred = logistic.predict(X_test_tfidf)\n\n# 분류 평가지표\nprint(classification_report(tfidf_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.33      0.55      0.41        65\n           1       0.59      0.70      0.64        64\n           2       0.49      0.68      0.57        53\n           3       0.38      0.74      0.50        57\n           4       0.19      1.00      0.32         6\n           5       0.50      0.83      0.62        35\n           6       0.62      0.84      0.71        38\n           7       0.00      0.00      0.00         0\n           8       0.41      0.35      0.38       236\n           9       0.76      0.44      0.56       446\n\n    accuracy                           0.50      1000\n   macro avg       0.43      0.61      0.47      1000\nweighted avg       0.58      0.50      0.51      1000\n\n\n\n\n\n\nSentenceTransformer방식은 시간이 오래 걸려 사전 처리된 데이터를 불러와 사용하였다.\n\nimport pickle\n## pkl 파일 불러오기기\ntest_pkl_path = \"/home/sungil/DXschool/data/X_test_setence_transformer3.pkl\"\ntrain_pkl_path = \"/home/sungil/DXschool/data/X_train_setence_transformer3.pkl\"\n\nwith open(train_pkl_path,\"rb\") as f:\n    X_train_embedding = pickle.load(f)\n\nwith open(test_pkl_path,\"rb\") as f:\n    X_test_embedding = pickle.load(f)\n\n\nX_train_embedding = pd.DataFrame(X_train_embedding[:2700])\nX_test_embedding = pd.DataFrame(X_test_embedding[:1000])\n# 차원 수\nX_train_embedding.shape, X_test_embedding.shape\n\n((2700, 768), (1000, 768))\n\n\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_embedding, y_train)\n\n# 예측\nembedding_pred = logistic.predict(X_test_embedding)\n\n# 분류 평가지표\nprint(classification_report(embedding_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.59      0.60      0.60       106\n           1       0.63      0.55      0.59        88\n           2       0.70      0.66      0.68        79\n           3       0.57      0.61      0.59       103\n           4       0.38      0.52      0.44        23\n           5       0.60      0.66      0.63        53\n           6       0.62      0.65      0.63        49\n           7       0.07      0.15      0.09        13\n           8       0.52      0.47      0.49       219\n           9       0.60      0.58      0.59       267\n\n    accuracy                           0.57      1000\n   macro avg       0.53      0.55      0.53      1000\nweighted avg       0.58      0.57      0.57      1000\n\n\n\n\n\n\n\n\n\n\n\nimage-2.png\n\n\n분류 결과\n\nacurracy (정확도) : 0.5, 0.5, 0.57\nprecision (정밀도) : 0.43, 0.43, 0.53\nrecall (재현률) : 0.59, 0.61, 0.55\n\n세 모델 모두 분류 성능이 좋지 않다는걸 확인할 수 있다.\ntrain 데이터의 클래스 불균형(4번,7번 클래스가 특히 부족)이 있었다.\n클래스 불균형 문제를 해결하거나, 하이퍼파라미터 튜닝, 다른 분류모델 사용, 다른 벡터화 기법 사용 등을 고려해봐야 한다.\n\n\n\n전처리 과정을 묶고 최적의 파라미터 찾기\n\n\n텍스트 데이터를 숫자 벡터로 변환하고, 분류 모델을 평가하는 과정을 파이프라인을 통해 하나로 묶을 수 있다.\nPipeline([(\"이름1\", 클래스1),(\"이름2\", 클래스2)])\n다음 파이프라인은 형태소로 분리한 텍스트 데이터를 받아 - TF-IDF 방식 숫자형 벡터로 변환 - 로지스틱 회귀분석\n을 진행한다.\n\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline(\n    [\n        (\"tf_idf\", TfidfVectorizer()),\n        (\"logi\", LogisticRegression())\n    ]\n)\n\n\n# 학습\npipeline.fit(konlpy_morphs_train,y_train)\n\nPipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])TfidfVectorizerTfidfVectorizer()LogisticRegressionLogisticRegression()\n\n\n\n# 평가 (Accuracy)\npipeline.score(konlpy_morphs_test, y_test)\n\n0.5\n\n\n\n\n\n하이퍼 파라미터는 파라미터와 구분하여 사용자가 설정하는 값들을 모두 지칭한다.\nGridSearchCV를 사용하여 최적의 하이퍼파라미터 조합을 찾아낸다.\n숫자벡터 변환, 분류 모델 들어가는 하이파파라미터들의 조합은 아주 많다. 하나하나 하기엔 오래걸리니까 한다.\n앞서 만든 파이프라인을 사용한다.\n{“이름1__파라미터명1” : [파라미터1,파라미터2…], “이름1__파라미터명2” : [파라미터1,파라미터2…], “이름2__파라미터명1” : [파라미터1,파라미터2…]}\n으로 작성하고\nGridSearchCV(pipeline, grid_params, cv=3)\n\npipeline : 파이프라인\ngrid_params : 파리미터들\ncv : Cross Validation\n\n이건 Cross Validation(교차 검증)이다.\n학습 데이터를 n개로 나눠서 n번 학습을 하고 자체 평가를 한다.\n성능이 좋아지지만, 시간이 오래 걸린디.\n\n\n\nimage.png\n\n\nLogistic Regression의 C파라미터는 규제 강도를 조절한다.\n작을수록 규제가 강해지고 높을수록 약해진다.\n\n규제가 강하면 과대적합을 방지하지만 과소적합의 위험이 있다.\n규제가 약하면 과소적합을 방지하지만 과대적합의 위험이 있다.\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    \"tf_idf__ngram_range\" : [(1,1),(1,2),(1,3)],\n    \"tf_idf__max_df\" : [0.5, 0.7, 0.8],\n    \"tf_idf__min_df\" : [10, 20, 30],\n    \"logi__C\" : [0.001, 0.1, 1, 10],\n}\n\ngridsearch = GridSearchCV(\n    pipeline,\n    grid_params,\n    cv = 3\n)\n\ngridsearch.fit(konlpy_morphs_train,y_train) \n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('tf_idf', TfidfVectorizer()),\n                                       ('logi', LogisticRegression())]),\n             param_grid={'logi__C': [0.001, 0.1, 1, 10],\n                         'tf_idf__max_df': [0.5, 0.7, 0.8],\n                         'tf_idf__min_df': [10, 20, 30],\n                         'tf_idf__ngram_range': [(1, 1), (1, 2), (1, 3)]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('tf_idf', TfidfVectorizer()),\n                                       ('logi', LogisticRegression())]),\n             param_grid={'logi__C': [0.001, 0.1, 1, 10],\n                         'tf_idf__max_df': [0.5, 0.7, 0.8],\n                         'tf_idf__min_df': [10, 20, 30],\n                         'tf_idf__ngram_range': [(1, 1), (1, 2), (1, 3)]})estimator: PipelinePipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])TfidfVectorizerTfidfVectorizer()LogisticRegressionLogisticRegression()\n\n\n총 조합 수는 - n_gram_range 3개 - max_df 3개 - min_df 3개 - C 4개\n으로 3x3x3x4 = 108개의 조합을 시도하고, cv = 3이기 때문에 각 조합마다 3번씩 학습, 총 324번을 학습한다. 시간이 오래 걸리는 이유이다.\nbest_params_ 메소드를 통해 최적의 하이퍼파라미터를 출력한다.\nbest_estimator_ 메소드로 최적의 파라미터로 학습한 모델을 불러온다.\n\ngridsearch.best_params_\n\n{'logi__C': 1,\n 'tf_idf__max_df': 0.5,\n 'tf_idf__min_df': 10,\n 'tf_idf__ngram_range': (1, 1)}\n\n\n\nbest_model = gridsearch.best_estimator_\n\n# 최적의 파라미터 모델 예측\nbest_param_pred = best_model.predict(konlpy_morphs_test)\n\n# 예측 결과 지표\nprint(classification_report(y_test, best_param_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.55      0.32      0.40       109\n           1       0.71      0.62      0.66        76\n           2       0.70      0.50      0.58        74\n           3       0.76      0.38      0.51       111\n           4       1.00      0.19      0.32        32\n           5       0.83      0.52      0.64        58\n           6       0.80      0.63      0.71        52\n           7       0.00      0.00      0.00        30\n           8       0.34      0.40      0.37       199\n           9       0.44      0.76      0.56       259\n\n    accuracy                           0.51      1000\n   macro avg       0.61      0.43      0.47      1000\nweighted avg       0.55      0.51      0.49      1000\n\n\n\n0.51로 여전히 낮다.\n\n\n\n\n어떤 단어가 분류에 가중치를 많이 주었는지 분류 모델을 통해 시각화 해보았다.\n10 * 358 (혐오대상, 단어)의 가중치 행렬이다.\n\nweight = best_model.steps[1][1].coef_\nweight.shape\n\n(10, 358)\n\n\n혐오 대상별 단어 시각화 함수\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport squarify\n\ndef plot_x(x):\n    vocab = best_model.steps[0][1].vocabulary_\n    weight = best_model.steps[1][1].coef_\n    df = pd.DataFrame([vocab.keys(), vocab.values()]).T\n    df.sort_values(by=1, inplace=True)\n\n    df[\"W\"] = weight[x]\n    df.columns = ['단어', \"인덱스\", \"가중치\"]\n    df.set_index('인덱스', inplace=True)\n\n    print(df.sort_values('가중치', ascending=False).head())\n    fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n    font_name = fm.FontProperties(fname=fontpath, size=10).get_name()\n    plt.rc('font', family=font_name)\n\n    plt.rcParams['figure.figsize'] = (10,10)\n    plt.rcParams['font.size'] = 15\n\n    sizes = df.sort_values(by='가중치', ascending=False).head(30)['가중치']\n    labels= df.sort_values(by='가중치', ascending=False).head(30)['단어']\n\n    norm = matplotlib.colors.Normalize(vmin=min(sizes),vmax=max(sizes))\n    colors = [matplotlib.cm.Reds(norm(value)) for value in sizes]\n\n    squarify.plot(sizes, 10, 10, label=labels, color=colors,bar_kwargs=dict(linewidth=8, edgecolor=\"#eee\"))\n    plt.title(\"혐오에 활용되는 주요 키워드\")\n    plt.axis('off')\n    plt.show()\n\n\nfor i,j in enumerate(train.loc[:, \"여성/가족\":\"clean\"].columns):\n    print(f\"인덱스[{i}] : {j}\")\n\n인덱스[0] : 여성/가족\n인덱스[1] : 남성\n인덱스[2] : 성소수자\n인덱스[3] : 인종/국적\n인덱스[4] : 연령\n인덱스[5] : 지역\n인덱스[6] : 종교\n인덱스[7] : 기타 혐오\n인덱스[8] : 악플/욕설\n인덱스[9] : clean\n\n\n\n\n\nplot_x(0)\n\n     단어       가중치\n인덱스              \n347  한남  6.890850\n281  자지  3.586034\n93   댕이  3.065344\n69   냄져  3.052125\n253  이기  2.174341\n\n\n\n\n\n\n\n\n\nplot_x(1)\n\n     단어       가중치\n인덱스              \n347  한남  6.890850\n281  자지  3.586034\n93   댕이  3.065344\n69   냄져  3.052125\n253  이기  2.174341\n\n\n\n\n\n\n\n\n\nplot_x(2)\n\n       단어       가중치\n인덱스                \n100   동성애  4.647741\n101  동성애자  3.780582\n23     게이  3.533253\n58     꼬충  3.243192\n110    똥꼬  2.903039\n\n\n\n\n\n\n\n\n\nplot_x(3)\n\n      단어       가중치\n인덱스               \n309  조선족  3.923741\n316   중국  3.445646\n84   다문화  3.265852\n65    난민  2.721175\n333  탈북자  2.499918\n\n\n\n\n\n\n\n\n\nplot_x(4)\n\n      단어       가중치\n인덱스               \n50    급식  4.551949\n64    나이  1.496709\n169   새끼  1.333710\n236  올라오  1.199835\n204   안되  1.141374\n\n\n\n\n\n\n\n\n\nplot_x(5)\n\n      단어       가중치\n인덱스               \n293  전라도  4.621232\n355   홍어  4.062138\n193   쌍도  3.432531\n90    대구  2.994224\n26   경상도  2.916873\n\n\n\n\n\n\n\n\n\nplot_x(6)\n\n     단어       가중치\n인덱스              \n19   개독  3.961684\n127  목사  3.946740\n186  슬람  3.435409\n312  종교  3.332901\n31   교회  2.903494\n\n\n\n\n\n\n\n\n\nplot_x(7)\n\n      단어       가중치\n인덱스               \n159  빨갱이  2.602123\n325   차별  1.430154\n231   역시  1.243613\n184   수준  1.189651\n49   금지법  1.188665\n\n\n\n\n\n\n\n\n\nplot_x(8)\n\n     단어       가중치\n인덱스              \n195  씨받  1.576897\n297  정권  1.286304\n319  지랄  1.248540\n15   가지  1.229520\n188  시발  1.163751\n\n\n\n\n\n\n\n\n\nplot_x(9)\n\n      단어       가중치\n인덱스               \n10   ㅂ니다  1.375340\n244   원래  1.253776\n71    네요  1.123729\n137   미국  1.089941\n146   범죄  0.916220"
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#텍스트-데이터의-전처리",
    "href": "posts/텍스트마이닝_sample.html#텍스트-데이터의-전처리",
    "title": "텍스트마이닝",
    "section": "",
    "text": "토큰화란 텍스트 데이터 처리에서 중요한 첫 단계로 텍스트를 분석하기 위해 작은 단위로 분할하는 과정을 의미한다.\n토큰화의 목적은 텍스트를 단어, 문장, 구절 등 의미 있는 작은 단위로 나누어 기계가 이해할 수 있게 하는 것이다.\n\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_data = 'data/unsmile_train_v1.0.tsv'\ntrain = pd.read_csv(train_data, delimiter = '\\t')\n\ntest_data = 'data/unsmile_valid_v1.0.tsv'\ntest = pd.read_csv(test_data, delimiter = '\\t')\ntrain.head()\n\n\n\n\n\n\n\n\n문장\n여성/가족\n남성\n성소수자\n인종/국적\n연령\n지역\n종교\n기타 혐오\n악플/욕설\nclean\n개인지칭\n\n\n\n\n0\n일안하는 시간은 쉬고싶어서 그런게 아닐까\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o doin 진짜 띵...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nntlk는 자연어 처리에 사용되는 패키지이고, word_tokenize는 텍스트를 단어 단위로 분할하는데 사용된다.\npunkt는 ntlk에서 제공되는 패키지로, 텍스트를 토큰화하는데 필요한 알고리즘과 데이터가 있다.\n\nfrom nltk import word_tokenize\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /home/sungil/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\n“넌 바라는게 너무 많아, 우리 헤어져” 를 word_tokenize()로 토큰화하면 다음과 같다.\n\nwords = word_tokenize(\"넌 바라는게 너무 많아, 우리 헤어져\")\nwords\n\n['넌', '바라는게', '너무', '많아', ',', '우리', '헤어져']\n\n\n전체 문장 토큰화\n\nclean_token = []\nfor i in train['문장']:\n    for j in word_tokenize(i):\n        clean_token.append(j)\n# 9개 값만 뽑아보기\nclean_token[:9]\n\n['일안하는', '시간은', '쉬고싶어서', '그런게', '아닐까', '아동성범죄와', '페도버는', '기록바', '끊어져']\n\n\n토큰화된 텍스트 데이터에서 어떤 단어가 많이 나왔는지, 빈도를 세어 보고, 워드클라우드를 그려보았다.\nCounter() : 딕셔너리 형식으로 리스트 내의 값의 빈도 출력\n\nfrom collections import Counter\ncount = Counter(clean_token)\n# 상위 15개 값만 추출\ncount.most_common(15)\n\n[('?', 3815),\n ('.', 3416),\n ('!', 2066),\n (',', 1174),\n ('..', 979),\n ('...', 905),\n ('다', 667),\n ('진짜', 543),\n ('왜', 540),\n ('ㅋㅋ', 435),\n ('존나', 376),\n ('그냥', 372),\n ('더', 368),\n ('ㅋㅋㅋ', 363),\n ('&gt;', 296)]\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwc = WordCloud(background_color = 'white', font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\nwc.generate_from_frequencies(count)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n\n\n\n빈도를 세본 결과, 1글자(왜,다,더,좀..)와 “ㅋㅋㅋ”등 큰 의미가 없는 단어가 있었다.\n\n\n\n한글자 단어와, 기호, “ㅋㅋㅋ” 등 큰 의미가 없는 단어를 제거하면,\n텍스트 데이터의 노이즈를 줄이고, 더 정확하고 유용한 정보를 추출할 수 있다.\n\n텍스트 데이터의 전처리를 위해 re패키지를 사용한다.\n\nre패키지는 정규표현식을 사용하여 문자열을 조작하는 기능을 제공한다.\np = re.compile(pattern)은 토큰에 pattern이 포함되어 있는지를 출력한다(True/False)\n\"[ㅋㅎㄷㅇ~!?.\\\\-ㅡ0-9a-z]+\" 패턴은 ㅋ, ㅎ, ㄷ, ㅇ, ~, !, ?, ., -, ㅡ, 숫자(0~9), 소문자 영어(a~z) 를 의미한다.\n\n\n\n반복문을 사용하여 공백 기준으로 토큰화\n토큰이 한 글자거나 pattern에 해당하면 스킵\n스킵되지 않은 토큰들을 다시 문장으로 만들어 train clean 리스트에 넣기\n\n\nimport re\np = re.compile(\"[ㅋㅎㄷㅇ~!?.\\\\-ㅡ0-9a-z]+\")\ntrain_clean = [] \n\nfor doc in train['문장']:\n    temp = []\n    for token in doc.split(\" \"):\n        if len(token) &lt; 2:\n            continue\n        if p.search(token):\n            continue\n\n        temp.append(token)\n    train_clean.append(\" \".join(temp))\n\n# 테스트 데이터\ntest_clean = []\n\nfor doc in test['문장']:\n    temp = []\n    for token in doc.split(\" \"):\n        if len(token) &lt; 2:\n            continue\n        if p.search(token):\n            continue\n\n        temp.append(token)\n    test_clean.append(\" \".join(temp))\n\n텍스트 데이터가 더 깔끔해진 것을 확인할 수 있다.\n\ntrain_clean[:6]\n\n['일안하는 시간은 쉬고싶어서 그런게 아닐까',\n '아동성범죄와 페도버는 기록바 끊어져 영원히 고통 무슬림 IQ 떨어지고 출산 위험은',\n '루나 솔로앨범 나왔을 때부터 머모 기운 있었음 진짜 띵곡임 들어보셈\"',\n '홍팍에도 어버이연합인가 보내요 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽에서 고소',\n '아놔 여기 댓들은 여자들이 김치녀라고 먼저 여자들은 심하게 이렇게 내가 둘다 나쁜 이상도 이하도 아닌데',\n '고향가서 피방가면 동네 부럴 친구들이랑은 거르는 없이 이야기하니까 말하게 되더라 당연히 키보드를 치거나 그러지는 않는데 말하는게 많이 거칠어지긴 반성해야겠네']\n\n\n\n\n\n한국어 문법 특성상 텍스트 데이터를 공백으로 나누는 것보다, 형태소 단위로 나누면 텍스트의 의미를 더 정밀하게 분석할 수 있다.\n\n형태소 분리를 하기 위해 konlpy패키지를 사용한다.\nkonlpy 패키지에는 Hannanum, Kkma, Komoran, Mecab, Okt등의 형태소 분석기가 있다.\nmorphs()함수는 문장을 형태소 단위로 나누어준다.\n다음은 형태소 단위 분리의 예시이다.\n\nfrom konlpy.tag import Okt, Kkma\nokt = Okt()\n\nt_1 = \"아버지가 방에 들어가신다\"\nt_2 = \"아버지 가방에 들어가신다\"\n\nprint(okt.morphs(t_1))\nprint(okt.morphs(t_2))\n\n['아버지', '가', '방', '에', '들어가신다']\n['아버지', '가방', '에', '들어가신다']\n\n\n형태소 단위 분리에 앞서 emoji패키지를 사용해 문장 내의 이모지를 제거하였다.\n\nimport emoji\ntrain_clean = [emoji.replace_emoji(tc) for tc in train_clean]\ntest_clean = [emoji.replace_emoji(tc) for tc in test_clean]\n\nKkma형태소 분석기의 morphs() 함수를 사용하여 문장을 형태소로 분리, 리스트 형태로 저장.\ntrain : test = 2700 : 1000 으로 사용\n\nfrom tqdm import tqdm\n\nkkma = Kkma()\nkonlpy_morphs_train = []\nfor doc in tqdm(train_clean[:2700]):\n  rs = kkma.morphs(doc)\n  konlpy_morphs_train.append(\" \".join(rs))\n\n# 테스트트\nkonlpy_morphs_test = []\nfor doc in tqdm(test_clean[:1000]): \n  rs = kkma.morphs(doc)\n  konlpy_morphs_test.append(\" \".join(rs))\n\n  0%|          | 0/2700 [00:00&lt;?, ?it/s]100%|██████████| 2700/2700 [00:37&lt;00:00, 72.91it/s] \n100%|██████████| 1000/1000 [00:12&lt;00:00, 81.21it/s]\n\n\n\nkonlpy_morphs_train[:6]\n\n['일 안 하 는 시간 은 쉬 고 싶 어서 그러 ㄴ 것 이 아니 ㄹ까',\n '아동 성 범죄 와 페도버 는 기록 바 끊어지 어 영원히 고통 무슬림 IQ 떨어지 고 출산 위험 은',\n '루 나 솔로 앨범 나오 았 을 때 부터 머 모 기운 있 었 음 진짜 띵 곡 임 듣 어 보셈 \"',\n '홍팍에 도 어버이 연합 이 ㄴ가 보 내요 이러 ㄴ 뎃 글 있 는데 이거 어버이 연합 측 에 신고 하면 그쪽 에서 고소',\n '아 아 노 아 여기 댓 듣 은 여자 들 이 김치 녀 라고 먼저 여자 들 은 심하 게 이렇 게 내가 둘 다 나쁘 ㄴ 이상 도 이하 도 아니 ㄴ데',\n '고향 가 아서 피 방가 이 면 동네 부 럴 치 ㄴ 구들 이랑 은 거르 는 없이 이야기 하 니까 말하 게 되 더라 당연히 키보드 를 치 거나 그러 지 는 않 는데 말하 는 것 이 많이 거칠어지 기 는 반성 하 어야 겠네']"
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#텍스트-데이터-변환",
    "href": "posts/텍스트마이닝_sample.html#텍스트-데이터-변환",
    "title": "텍스트마이닝",
    "section": "",
    "text": "kkma 형태소 분석기로 분리한 텍스트 데이터를, 모델에 적용하기 위해 숫자 형태로 바꾸어준다.\n\n\n\nBOW(Bag-of-Words) 는 텍스트 데이터에서 각 단어의 빈도를 세어 숫자형 벡터로 변환하는 방법이다.\nCountVectorizer는 BOW방식으로 텍스트 데이터를 숫자형 피처 벡터로 변환해주는 Scikit-learn의 클래스이다.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\nsample_bow = CountVectorizer()\nsample_bow.fit(sample_text)\n\nCountVectorizer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizerCountVectorizer()\n\n\n예시 문장으로 만든 단어 사전 sample_bow는 다음과 같다.\n\nsample_bow.vocabulary_\n\n{'나는': 1,\n '오늘': 9,\n '치킨을': 13,\n '먹을거야': 4,\n '너는': 2,\n '저녁에': 12,\n '무엇을': 5,\n '먹을거니': 3,\n '아침에': 7,\n '운동을': 11,\n '하고': 14,\n '왔어': 10,\n '어제': 8,\n '했더니': 15,\n '배가': 6,\n '고프다': 0}\n\n\n단어사전 16글자에서 어떤 단어가 등장했는지를 1로 표현하는 숫자 벡터이다.\n데이터프레임으로 변환한 단어사전과 비교해보면 이해할 수 있다.\n\npd.DataFrame([[i,j] for i,j in sample_bow.vocabulary_.items()], columns=[\"단어\",\"인덱스\"]).sort_values(\"인덱스\").reset_index(drop = True).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n단어\n고프다\n나는\n너는\n먹을거니\n먹을거야\n무엇을\n배가\n아침에\n어제\n오늘\n왔어\n운동을\n저녁에\n치킨을\n하고\n했더니\n\n\n인덱스\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n\n\n\n\nrs = sample_bow.transform(sample_text)\nrs.toarray()\n\narray([[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1]])\n\n\n\n\n\nTF-IDF는 텍스트를 숫자형 벡터로 변환하는 방법 중 하나로, 단어의 빈도(Term Frequency)에 가중치를 부여하는 방법이다.\n단순히 단어의 빈도만을 사용하는 Bag-of-Words 모델보다 더 유용한 정보를 제공한다.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\ntfidf_vectorizer = TfidfVectorizer()\n\ntfidf_vector = tfidf_vectorizer.fit_transform(sample_text)\n\n\ntfidf_vectorizer.vocabulary_\n\n{'나는': 1,\n '오늘': 9,\n '치킨을': 13,\n '먹을거야': 4,\n '너는': 2,\n '저녁에': 12,\n '무엇을': 5,\n '먹을거니': 3,\n '아침에': 7,\n '운동을': 11,\n '하고': 14,\n '왔어': 10,\n '어제': 8,\n '했더니': 15,\n '배가': 6,\n '고프다': 0}\n\n\n단어사전에 해당하는 인덱스가 0,1 이 아닌 가중치가 부여됨을 확인할 수 있다.\n\npd.DataFrame([[i,j] for i,j in tfidf_vectorizer.vocabulary_.items()], columns=[\"단어\",\"인덱스\"]).sort_values(\"인덱스\").reset_index(drop = True).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n단어\n고프다\n나는\n너는\n먹을거니\n먹을거야\n무엇을\n배가\n아침에\n어제\n오늘\n왔어\n운동을\n저녁에\n치킨을\n하고\n했더니\n\n\n인덱스\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n\n\n\n\nprint(tfidf_vector.toarray())\n\n[[0.         0.4530051  0.         0.         0.57457953 0.\n  0.         0.         0.         0.36674667 0.         0.\n  0.         0.57457953 0.         0.        ]\n [0.         0.         0.49819711 0.49819711 0.         0.49819711\n  0.         0.         0.         0.31799276 0.         0.\n  0.39278432 0.         0.         0.        ]\n [0.         0.36559366 0.         0.         0.         0.\n  0.         0.46370919 0.         0.29597957 0.46370919 0.36559366\n  0.         0.         0.46370919 0.        ]\n [0.43671931 0.         0.         0.         0.         0.\n  0.43671931 0.         0.43671931 0.         0.         0.34431452\n  0.34431452 0.         0.         0.43671931]]\n\n\n\n\n\nSentenceTransformer는 Transformer(영화아님, 딥러닝 모델) 기반의 사전 학습된 모델을 통하여 문장을 고차원 숫자 벡터로 변환한다.\nTF-IDF는 단어의 빈도에 가중치를 부여하는 방식으로, 문맥을 고려하지 못한다. ex)똥맛 카레, 카레맛 똥\nSentenceTransformer는 Transformer기반 모델을 통해 문맥을 고려하여 문장 내의 단어 순서와 의미를 반영할 수 있다.\n차원수의 차이 때문에 단순한 텍스트 분석인지, 복잡한 자연어처리 작업인지에 따라 사용하면 된다.\n사용모델 paraphrase-MiniLM-L6-v2\n\nfrom sentence_transformers import SentenceTransformer\n\nsample_text = [\"나는 오늘 치킨을 먹을거야\",\n               \"너는 오늘 저녁에 무엇을 먹을거니\",\n               \"나는 오늘 아침에 운동을 하고 왔어\",\n               \"어제 저녁에 운동을 했더니 배가 고프다\"]\n\nsample_embedding = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\nembeddings = sample_embedding.encode(sample_text)\nembeddings\n\n2024-07-30 17:28:40.050781: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-07-30 17:28:40.051695: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-30 17:28:40.054281: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-30 17:28:40.061751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 17:28:40.074227: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 17:28:40.077846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-07-30 17:28:40.087182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-07-30 17:28:41.445550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\narray([[-0.0055915 ,  0.58696157,  0.12903269, ..., -0.5404554 ,\n         0.9031779 ,  0.508091  ],\n       [ 0.14074272,  0.4005823 ,  0.07539973, ..., -0.30134737,\n         0.9708954 ,  0.6109679 ],\n       [ 0.26552293,  0.540008  ,  0.1140722 , ..., -0.55974346,\n         0.8789244 ,  0.6666612 ],\n       [ 0.23060054,  0.45907632,  0.08265256, ..., -0.41313243,\n         1.0001894 ,  0.5942588 ]], dtype=float32)\n\n\n4 * 384의 행렬으로 변환되었다. (TF-IDF보다 큼)\n\nembeddings.shape\n\n(4, 384)"
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#모델-적용-분류-classification",
    "href": "posts/텍스트마이닝_sample.html#모델-적용-분류-classification",
    "title": "텍스트마이닝",
    "section": "",
    "text": "분류 모델을 사용하여 숫자 벡터로 바꾼 텍스트 데이터가 무얼 혐오하는지 분류한다.\n앞서 사용한 텍스트 데이터의 변환 중 BOW(CountVectorizer), TF-IDF(TfidfVectorizer) 방식을 사용한다.\n파라미터의 의미는 다음과 같다.\n\nstop_words= : 숫자 벡터로 변환하는 과정에서 무시할 단어의 리스트\nngram_range= : 단어 묶음의 범위\n\nngram_range=1이면 [“오늘”, “치킨을”]\nngram_range=2 면 [“오늘 치킨을”]\n(1,2)는 [“오늘”, “치킨을”, “오늘 치킨을”] 이다\n\nmax_df= 특정 단어가 너무 자주 등장하면 무시하는 기능 (0.7 -&gt; 전체 문서의 70%이상이면 무시)\nmin_df= 특정 단어가 너무 적게 등장하면 무시하는 기능 (10 -&gt; 10개 이하는 무시)\n\n\n\n\nunsmile_bow = CountVectorizer(stop_words=[\"으로\",\"을\",\"를\",\"은\",\"이다\"],\n                              ngram_range = (1,2),\n                              max_df = 0.7, min_df = 10)\nunsmile_tfidf = TfidfVectorizer(stop_words=[\"으로\",\"을\",\"를\",\"은\",\"이다\"],\n                              ngram_range = (1,2),\n                              max_df = 0.7, min_df = 10)\n\n앞서 konlpy.Kkma를 사용해서 형태소 단위로 나는 데이터를 사용한다.\nfit_transform()메소드로 train 데이터를 학습하고, 변환하고\ntransform() 메소드로 test 데이터를 변환한다.\n\nX_train_bow = unsmile_bow.fit_transform(konlpy_morphs_train)\nX_test_bow = unsmile_bow.transform(konlpy_morphs_test)\n\nX_train_tfidf = unsmile_tfidf.fit_transform(konlpy_morphs_train)\nX_test_tfidf = unsmile_tfidf.transform(konlpy_morphs_test)\n\ny는 어떤 대상을 혐오한지의 인덱스 값이다.\n\nfor i,j in enumerate(train.loc[:, \"여성/가족\":\"clean\"].columns):\n    print(f\"인덱스[{i}] : {j}\")\n\n인덱스[0] : 여성/가족\n인덱스[1] : 남성\n인덱스[2] : 성소수자\n인덱스[3] : 인종/국적\n인덱스[4] : 연령\n인덱스[5] : 지역\n인덱스[6] : 종교\n인덱스[7] : 기타 혐오\n인덱스[8] : 악플/욕설\n인덱스[9] : clean\n\n\n\ny_train = train.loc[:, \"여성/가족\":\"clean\"].values.argmax(axis=1)\ny_test = test.loc[:, \"여성/가족\":\"clean\"].values.argmax(axis=1)\ny_train = y_train[:2700]\ny_test = y_test[:1000]\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_bow, y_train)\n\n# 예측\nbow_pred = logistic.predict(X_test_bow)\n\n# 분류 평가지표\nprint(classification_report(bow_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.35      0.49      0.41        78\n           1       0.59      0.69      0.64        65\n           2       0.51      0.64      0.57        59\n           3       0.43      0.73      0.54        66\n           4       0.19      1.00      0.32         6\n           5       0.55      0.76      0.64        42\n           6       0.58      0.81      0.67        37\n           7       0.00      0.00      0.00         5\n           8       0.38      0.34      0.36       218\n           9       0.74      0.45      0.56       424\n\n    accuracy                           0.50      1000\n   macro avg       0.43      0.59      0.47      1000\nweighted avg       0.57      0.50      0.51      1000\n\n\n\n\n\n\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_tfidf, y_train)\n\n# 예측\ntfidf_pred = logistic.predict(X_test_tfidf)\n\n# 분류 평가지표\nprint(classification_report(tfidf_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.33      0.55      0.41        65\n           1       0.59      0.70      0.64        64\n           2       0.49      0.68      0.57        53\n           3       0.38      0.74      0.50        57\n           4       0.19      1.00      0.32         6\n           5       0.50      0.83      0.62        35\n           6       0.62      0.84      0.71        38\n           7       0.00      0.00      0.00         0\n           8       0.41      0.35      0.38       236\n           9       0.76      0.44      0.56       446\n\n    accuracy                           0.50      1000\n   macro avg       0.43      0.61      0.47      1000\nweighted avg       0.58      0.50      0.51      1000\n\n\n\n\n\n\nSentenceTransformer방식은 시간이 오래 걸려 사전 처리된 데이터를 불러와 사용하였다.\n\nimport pickle\n## pkl 파일 불러오기기\ntest_pkl_path = \"/home/sungil/DXschool/data/X_test_setence_transformer3.pkl\"\ntrain_pkl_path = \"/home/sungil/DXschool/data/X_train_setence_transformer3.pkl\"\n\nwith open(train_pkl_path,\"rb\") as f:\n    X_train_embedding = pickle.load(f)\n\nwith open(test_pkl_path,\"rb\") as f:\n    X_test_embedding = pickle.load(f)\n\n\nX_train_embedding = pd.DataFrame(X_train_embedding[:2700])\nX_test_embedding = pd.DataFrame(X_test_embedding[:1000])\n# 차원 수\nX_train_embedding.shape, X_test_embedding.shape\n\n((2700, 768), (1000, 768))\n\n\n\nlogistic = LogisticRegression()\n\n# 학습\nlogistic.fit(X_train_embedding, y_train)\n\n# 예측\nembedding_pred = logistic.predict(X_test_embedding)\n\n# 분류 평가지표\nprint(classification_report(embedding_pred,y_test))\n\n              precision    recall  f1-score   support\n\n           0       0.59      0.60      0.60       106\n           1       0.63      0.55      0.59        88\n           2       0.70      0.66      0.68        79\n           3       0.57      0.61      0.59       103\n           4       0.38      0.52      0.44        23\n           5       0.60      0.66      0.63        53\n           6       0.62      0.65      0.63        49\n           7       0.07      0.15      0.09        13\n           8       0.52      0.47      0.49       219\n           9       0.60      0.58      0.59       267\n\n    accuracy                           0.57      1000\n   macro avg       0.53      0.55      0.53      1000\nweighted avg       0.58      0.57      0.57      1000"
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#성능-지표-해석",
    "href": "posts/텍스트마이닝_sample.html#성능-지표-해석",
    "title": "텍스트마이닝",
    "section": "",
    "text": "image-2.png\n\n\n분류 결과\n\nacurracy (정확도) : 0.5, 0.5, 0.57\nprecision (정밀도) : 0.43, 0.43, 0.53\nrecall (재현률) : 0.59, 0.61, 0.55\n\n세 모델 모두 분류 성능이 좋지 않다는걸 확인할 수 있다.\ntrain 데이터의 클래스 불균형(4번,7번 클래스가 특히 부족)이 있었다.\n클래스 불균형 문제를 해결하거나, 하이퍼파라미터 튜닝, 다른 분류모델 사용, 다른 벡터화 기법 사용 등을 고려해봐야 한다."
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#파이프라인-하이퍼파라미터-튜닝",
    "href": "posts/텍스트마이닝_sample.html#파이프라인-하이퍼파라미터-튜닝",
    "title": "텍스트마이닝",
    "section": "",
    "text": "전처리 과정을 묶고 최적의 파라미터 찾기\n\n\n텍스트 데이터를 숫자 벡터로 변환하고, 분류 모델을 평가하는 과정을 파이프라인을 통해 하나로 묶을 수 있다.\nPipeline([(\"이름1\", 클래스1),(\"이름2\", 클래스2)])\n다음 파이프라인은 형태소로 분리한 텍스트 데이터를 받아 - TF-IDF 방식 숫자형 벡터로 변환 - 로지스틱 회귀분석\n을 진행한다.\n\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline(\n    [\n        (\"tf_idf\", TfidfVectorizer()),\n        (\"logi\", LogisticRegression())\n    ]\n)\n\n\n# 학습\npipeline.fit(konlpy_morphs_train,y_train)\n\nPipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])TfidfVectorizerTfidfVectorizer()LogisticRegressionLogisticRegression()\n\n\n\n# 평가 (Accuracy)\npipeline.score(konlpy_morphs_test, y_test)\n\n0.5\n\n\n\n\n\n하이퍼 파라미터는 파라미터와 구분하여 사용자가 설정하는 값들을 모두 지칭한다.\nGridSearchCV를 사용하여 최적의 하이퍼파라미터 조합을 찾아낸다.\n숫자벡터 변환, 분류 모델 들어가는 하이파파라미터들의 조합은 아주 많다. 하나하나 하기엔 오래걸리니까 한다.\n앞서 만든 파이프라인을 사용한다.\n{“이름1__파라미터명1” : [파라미터1,파라미터2…], “이름1__파라미터명2” : [파라미터1,파라미터2…], “이름2__파라미터명1” : [파라미터1,파라미터2…]}\n으로 작성하고\nGridSearchCV(pipeline, grid_params, cv=3)\n\npipeline : 파이프라인\ngrid_params : 파리미터들\ncv : Cross Validation\n\n이건 Cross Validation(교차 검증)이다.\n학습 데이터를 n개로 나눠서 n번 학습을 하고 자체 평가를 한다.\n성능이 좋아지지만, 시간이 오래 걸린디.\n\n\n\nimage.png\n\n\nLogistic Regression의 C파라미터는 규제 강도를 조절한다.\n작을수록 규제가 강해지고 높을수록 약해진다.\n\n규제가 강하면 과대적합을 방지하지만 과소적합의 위험이 있다.\n규제가 약하면 과소적합을 방지하지만 과대적합의 위험이 있다.\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    \"tf_idf__ngram_range\" : [(1,1),(1,2),(1,3)],\n    \"tf_idf__max_df\" : [0.5, 0.7, 0.8],\n    \"tf_idf__min_df\" : [10, 20, 30],\n    \"logi__C\" : [0.001, 0.1, 1, 10],\n}\n\ngridsearch = GridSearchCV(\n    pipeline,\n    grid_params,\n    cv = 3\n)\n\ngridsearch.fit(konlpy_morphs_train,y_train) \n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('tf_idf', TfidfVectorizer()),\n                                       ('logi', LogisticRegression())]),\n             param_grid={'logi__C': [0.001, 0.1, 1, 10],\n                         'tf_idf__max_df': [0.5, 0.7, 0.8],\n                         'tf_idf__min_df': [10, 20, 30],\n                         'tf_idf__ngram_range': [(1, 1), (1, 2), (1, 3)]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('tf_idf', TfidfVectorizer()),\n                                       ('logi', LogisticRegression())]),\n             param_grid={'logi__C': [0.001, 0.1, 1, 10],\n                         'tf_idf__max_df': [0.5, 0.7, 0.8],\n                         'tf_idf__min_df': [10, 20, 30],\n                         'tf_idf__ngram_range': [(1, 1), (1, 2), (1, 3)]})estimator: PipelinePipeline(steps=[('tf_idf', TfidfVectorizer()), ('logi', LogisticRegression())])TfidfVectorizerTfidfVectorizer()LogisticRegressionLogisticRegression()\n\n\n총 조합 수는 - n_gram_range 3개 - max_df 3개 - min_df 3개 - C 4개\n으로 3x3x3x4 = 108개의 조합을 시도하고, cv = 3이기 때문에 각 조합마다 3번씩 학습, 총 324번을 학습한다. 시간이 오래 걸리는 이유이다.\nbest_params_ 메소드를 통해 최적의 하이퍼파라미터를 출력한다.\nbest_estimator_ 메소드로 최적의 파라미터로 학습한 모델을 불러온다.\n\ngridsearch.best_params_\n\n{'logi__C': 1,\n 'tf_idf__max_df': 0.5,\n 'tf_idf__min_df': 10,\n 'tf_idf__ngram_range': (1, 1)}\n\n\n\nbest_model = gridsearch.best_estimator_\n\n# 최적의 파라미터 모델 예측\nbest_param_pred = best_model.predict(konlpy_morphs_test)\n\n# 예측 결과 지표\nprint(classification_report(y_test, best_param_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.55      0.32      0.40       109\n           1       0.71      0.62      0.66        76\n           2       0.70      0.50      0.58        74\n           3       0.76      0.38      0.51       111\n           4       1.00      0.19      0.32        32\n           5       0.83      0.52      0.64        58\n           6       0.80      0.63      0.71        52\n           7       0.00      0.00      0.00        30\n           8       0.34      0.40      0.37       199\n           9       0.44      0.76      0.56       259\n\n    accuracy                           0.51      1000\n   macro avg       0.61      0.43      0.47      1000\nweighted avg       0.55      0.51      0.49      1000\n\n\n\n0.51로 여전히 낮다."
  },
  {
    "objectID": "posts/텍스트마이닝_sample.html#결과",
    "href": "posts/텍스트마이닝_sample.html#결과",
    "title": "텍스트마이닝",
    "section": "",
    "text": "어떤 단어가 분류에 가중치를 많이 주었는지 분류 모델을 통해 시각화 해보았다.\n10 * 358 (혐오대상, 단어)의 가중치 행렬이다.\n\nweight = best_model.steps[1][1].coef_\nweight.shape\n\n(10, 358)\n\n\n혐오 대상별 단어 시각화 함수\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport squarify\n\ndef plot_x(x):\n    vocab = best_model.steps[0][1].vocabulary_\n    weight = best_model.steps[1][1].coef_\n    df = pd.DataFrame([vocab.keys(), vocab.values()]).T\n    df.sort_values(by=1, inplace=True)\n\n    df[\"W\"] = weight[x]\n    df.columns = ['단어', \"인덱스\", \"가중치\"]\n    df.set_index('인덱스', inplace=True)\n\n    print(df.sort_values('가중치', ascending=False).head())\n    fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n    font_name = fm.FontProperties(fname=fontpath, size=10).get_name()\n    plt.rc('font', family=font_name)\n\n    plt.rcParams['figure.figsize'] = (10,10)\n    plt.rcParams['font.size'] = 15\n\n    sizes = df.sort_values(by='가중치', ascending=False).head(30)['가중치']\n    labels= df.sort_values(by='가중치', ascending=False).head(30)['단어']\n\n    norm = matplotlib.colors.Normalize(vmin=min(sizes),vmax=max(sizes))\n    colors = [matplotlib.cm.Reds(norm(value)) for value in sizes]\n\n    squarify.plot(sizes, 10, 10, label=labels, color=colors,bar_kwargs=dict(linewidth=8, edgecolor=\"#eee\"))\n    plt.title(\"혐오에 활용되는 주요 키워드\")\n    plt.axis('off')\n    plt.show()\n\n\nfor i,j in enumerate(train.loc[:, \"여성/가족\":\"clean\"].columns):\n    print(f\"인덱스[{i}] : {j}\")\n\n인덱스[0] : 여성/가족\n인덱스[1] : 남성\n인덱스[2] : 성소수자\n인덱스[3] : 인종/국적\n인덱스[4] : 연령\n인덱스[5] : 지역\n인덱스[6] : 종교\n인덱스[7] : 기타 혐오\n인덱스[8] : 악플/욕설\n인덱스[9] : clean\n\n\n\n\n\nplot_x(0)\n\n     단어       가중치\n인덱스              \n347  한남  6.890850\n281  자지  3.586034\n93   댕이  3.065344\n69   냄져  3.052125\n253  이기  2.174341\n\n\n\n\n\n\n\n\n\nplot_x(1)\n\n     단어       가중치\n인덱스              \n347  한남  6.890850\n281  자지  3.586034\n93   댕이  3.065344\n69   냄져  3.052125\n253  이기  2.174341\n\n\n\n\n\n\n\n\n\nplot_x(2)\n\n       단어       가중치\n인덱스                \n100   동성애  4.647741\n101  동성애자  3.780582\n23     게이  3.533253\n58     꼬충  3.243192\n110    똥꼬  2.903039\n\n\n\n\n\n\n\n\n\nplot_x(3)\n\n      단어       가중치\n인덱스               \n309  조선족  3.923741\n316   중국  3.445646\n84   다문화  3.265852\n65    난민  2.721175\n333  탈북자  2.499918\n\n\n\n\n\n\n\n\n\nplot_x(4)\n\n      단어       가중치\n인덱스               \n50    급식  4.551949\n64    나이  1.496709\n169   새끼  1.333710\n236  올라오  1.199835\n204   안되  1.141374\n\n\n\n\n\n\n\n\n\nplot_x(5)\n\n      단어       가중치\n인덱스               \n293  전라도  4.621232\n355   홍어  4.062138\n193   쌍도  3.432531\n90    대구  2.994224\n26   경상도  2.916873\n\n\n\n\n\n\n\n\n\nplot_x(6)\n\n     단어       가중치\n인덱스              \n19   개독  3.961684\n127  목사  3.946740\n186  슬람  3.435409\n312  종교  3.332901\n31   교회  2.903494\n\n\n\n\n\n\n\n\n\nplot_x(7)\n\n      단어       가중치\n인덱스               \n159  빨갱이  2.602123\n325   차별  1.430154\n231   역시  1.243613\n184   수준  1.189651\n49   금지법  1.188665\n\n\n\n\n\n\n\n\n\nplot_x(8)\n\n     단어       가중치\n인덱스              \n195  씨받  1.576897\n297  정권  1.286304\n319  지랄  1.248540\n15   가지  1.229520\n188  시발  1.163751\n\n\n\n\n\n\n\n\n\nplot_x(9)\n\n      단어       가중치\n인덱스               \n10   ㅂ니다  1.375340\n244   원래  1.253776\n71    네요  1.123729\n137   미국  1.089941\n146   범죄  0.916220"
  },
  {
    "objectID": "posts/ezik_logistic_regression.html",
    "href": "posts/ezik_logistic_regression.html",
    "title": "이직 여부 로지스틱 회귀분석",
    "section": "",
    "text": "이거는 R코드에용~"
  },
  {
    "objectID": "posts/ezik_logistic_regression.html#더-정확한-결과를-위해-이상치-제거다중공선성-제거등의-방법이-필요하다.",
    "href": "posts/ezik_logistic_regression.html#더-정확한-결과를-위해-이상치-제거다중공선성-제거등의-방법이-필요하다.",
    "title": "이직 여부 로지스틱 회귀분석",
    "section": "더 정확한 결과를 위해 이상치 제거,다중공선성 제거등의 방법이 필요하다.",
    "text": "더 정확한 결과를 위해 이상치 제거,다중공선성 제거등의 방법이 필요하다."
  }
]